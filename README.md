# âœï¸ MNIST Digit Recognizer â€” Deep Learning Competition Project

![Python](https://img.shields.io/badge/Python-3.8+-blue?logo=python)
![Notebook](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)
![Kaggle](https://img.shields.io/badge/Kaggle-MNIST-blue?logo=kaggle)
![Accuracy](https://img.shields.io/badge/Accuracy-99.91%25-brightgreen)
![DL](https://img.shields.io/badge/Deep%20Learning-CNN%2C%20MLP%2C%20KNN-purple)

## ğŸ§¾ Overview

This project was developed as part of the **MNIST Digit Recognizer** competition on Kaggle. The goal was to build and evaluate deep learning models that can accurately classify handwritten digits from the famous MNIST dataset.

We implemented and compared three models:  
- ğŸ“ˆ **Convolutional Neural Network (CNN)**  
- ğŸ“Š **Multi-Layer Perceptron (MLP)**  
- ğŸ“ **K-Nearest Neighbors (KNN)**  

The CNN model achieved an outstanding accuracy of **99.91%**, thanks to a carefully designed architecture and optimized training strategies.

---

## ğŸ§  Problem Statement

The task is to classify images of handwritten digits (0â€“9) into their correct classes. The input consists of **28x28 grayscale images**, and the output is a single digit label.

---

## ğŸ—ƒï¸ Dataset

- ğŸ“¦ **Training Data**: 60,000 labeled images  
- ğŸ§ª **Test Data**: 10,000 unlabeled images  
- ğŸ“ Format: Each row is an image flattened into 784 pixels.  
- ğŸ§® Target: Digit (0â€“9)  
- Source: [Kaggle MNIST Competition](https://www.kaggle.com/c/digit-recognizer)

---

## âš™ï¸ Models & Architecture

### ğŸ§  CNN Architecture
- 2 Convolutional blocks (Conv2D + BatchNorm + MaxPooling + Dropout)
- Dense layers: [512 â 1024 â 10]
- Regularization: Batch Normalization, Dropout, EarlyStopping
- Optimizer: Adam + Learning Rate Scheduler

### ğŸ§  MLP Architecture
- Input: 512 â Hidden: 256 â 128 â Output: 10 (Softmax)
- Activation: ReLU, Optimizer: Adam
- Regularization: Dropout, EarlyStopping

### ğŸ§  KNN Classifier
- Classical algorithm used for benchmarking.
- GridSearchCV used to optimize `K` and distance metrics.

---

## ğŸ“ˆ Training & Optimization

- âœ… Optimizer: Adam with learning rate decay  
- ğŸ•¹ï¸ Callback: EarlyStopping  
- ğŸ“‰ Loss Function: Categorical Cross-Entropy  
- ğŸ”„ Validation: 5-Fold Cross Validation  

---

## ğŸ“Š Performance Metrics

| Model | Accuracy | F1-Score |
|-------|----------|----------|
| CNN   | **99.91%** | 0.998    |
| MLP   | ~98.5%    | ~0.97    |
| KNN   | ~96.4%    | ~0.95    |

---

## ğŸ” Visualizations

- ğŸ“‰ Training & Validation Loss & Accuracy Curves  
- ğŸ” Confusion Matrices  
- ğŸ¯ Classification Reports  

> These helped us identify class-specific misclassifications and ensure model stability.

---

## ğŸ§ª Lessons Learned

- CNNs excel in spatial pattern recognition for image tasks.
- Regularization techniques like Dropout, BatchNorm, and EarlyStopping are essential for generalization.
- Simple models (MLP, KNN) are good baselines but underperform CNNs.
- Hyperparameter tuning is crucial for squeezing out performance.

---

## ğŸ“Œ Final Notes

- This project was part of the academic year **2024â€“2025** deep learning coursework.
- Developed by:
  - ğŸ§‘â€ğŸ’» Mohamed Amine Tlili  
  - ğŸ§‘â€ğŸ’» Mohamed Aziz Loukil  
  - ğŸ§‘â€ğŸ’» Mohamed Laatar

ğŸ Achieved a **top-tier accuracy of 99.91%** on the MNIST test set â€” approaching human-level performance.

